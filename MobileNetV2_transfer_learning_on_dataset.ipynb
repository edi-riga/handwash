{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2 transfer learning on Hand Washing Video Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested on the following software setup:\n",
    "<ul>\n",
    "<li>Ubuntu 16.04.7 LTS</li>\n",
    "<li>Docker v.20.10.2</li>\n",
    "<li>TensorFlow docker container with TensorFlow 2.4.1 (container digest c3b4e83edf14) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn\n",
    "pip install h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to provide correct paths to the folders on your machine\n",
    "data_dir = pathlib.Path('/trainval')\n",
    "test_data_dir = pathlib.Path('/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of images for training and validation\n",
    "trainval_img_count = 0\n",
    "for subdir, dirs, files in os.walk(data_dir):\n",
    "    for image_file in files:\n",
    "        if image_file.endswith(\"jpg\"):\n",
    "            trainval_img_count += 1\n",
    "\n",
    "print('Number of trainval images: ', trainval_img_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of images in test dataset\n",
    "test_img_count = 0\n",
    "for subdir, dirs, files in os.walk(test_data_dir):\n",
    "    for image_file in files:\n",
    "        if image_file.endswith(\"jpg\"):\n",
    "            test_img_count += 1\n",
    "\n",
    "print('Number of images in the test dataset: ', test_img_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for the dataset loader. \n",
    "# Adjust batch size according to the memory volume of your GPU\n",
    "# 16 works well on most GPU\n",
    "# 256 works well on NVIDIA RTX 3090 with 24 GB VRAM\n",
    "batch_size = 16\n",
    "img_height = 224\n",
    "img_width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  label_mode='categorical',\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  label_mode='categorical',\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  test_data_dir,\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  label_mode='categorical',\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the names of the classes\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the dataset is imbalanced, is is necessary to get weights for each class\n",
    "\n",
    "# get the number of trainval images for each class\n",
    "images_by_labels = []\n",
    "for i in range(len(class_names)): \n",
    "    for subdir, dirs, files in os.walk(os.path.join(data_dir,str(i))):\n",
    "        n_of_files = 0\n",
    "        for image_file in files:\n",
    "            if image_file.endswith(\"jpg\"):\n",
    "                n_of_files += 1\n",
    "    images_by_labels.append(n_of_files)\n",
    "\n",
    "# calculate weights\n",
    "images_by_labels = np.array(images_by_labels)\n",
    "avg = np.average(images_by_labels)\n",
    "weights = avg / images_by_labels\n",
    "\n",
    "\n",
    "# create dictionary with weights as required for keras fit() function\n",
    "weights_dict = {}\n",
    "for item in range(len(weights)):\n",
    "    weights_dict[int(class_names[item])] = weights[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to improve performance, use buffered prefetching to load images \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_dataset = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale pixel values\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SIZE = (224, 224)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the convolutional base\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.008),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks to implement early stopping and saving the model\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint(monitor='val_accuracy', mode='max', \n",
    "                     verbose=1, save_freq='epoch', \n",
    "                     filepath='MobileNetV2_Handwashing_dataset.{epoch:02d}-{val_accuracy:.2f}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds,\n",
    "                    epochs=number_of_epochs,\n",
    "                    validation_data=val_ds,\n",
    "                    class_weight=weights_dict,\n",
    "                    callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualise accuracy\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the correct path to your model saved in h5 format\n",
    "path_to_saved_model = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load_model(path_to_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = saved_model.evaluate(test_ds)\n",
    "print('Test accuracy :', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
